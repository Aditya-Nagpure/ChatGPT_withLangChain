{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6019f2f-6cc2-4996-9a3d-9c1fce8e08f7",
   "metadata": {},
   "source": [
    "## Project: Implementing a ChatGPT App with LangChain from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a0d39b-4594-4fd0-9b2c-9603a2635ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in /Users/adi2/Library/Python/3.12/lib/python/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1382b592-b675-44f6-917c-707d7a2b9da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()  # By default, it looks for a `.env` file in the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ccc6baa-1ee7-4078-9fd1-fae2c8f645c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade -q langchain \n",
    "%pip install --upgrade -q langchain-community\n",
    "%pip install --upgrade -q openai \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef5ce28-19e5-44af-9086-753bff73ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your prompt:  Hello, How are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, estoy bien, ¿y tú cómo estás?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your prompt:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo', temperature=1)\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "#       SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        SystemMessage(content='You respond only in Spanish.'),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     prompt=prompt,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# if the code above gives an error use this:\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content.lower() in ['quit','q', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    response = chain.invoke({'content': content})\n",
    "    print(response)\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5fdb6-c8fc-4f65-9c52-0c24a7020eba",
   "metadata": {},
   "source": [
    "## Adding Chat Memory Using ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf77e64-7162-4f66-a511-abf82db198ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your prompt:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=False)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "# Create the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo', temperature=1)\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "])\n",
    "\n",
    "# Create chain using Runnable\n",
    "chain = RunnableMap({\n",
    "    \"chat_history\": memory.load_memory_variables,\n",
    "    \"content\": lambda x: x[\"content\"]\n",
    "}) | prompt | llm\n",
    "\n",
    "# Run loop\n",
    "while True:\n",
    "    content = input(\"Your prompt: \")\n",
    "    if content.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Invoke chain\n",
    "    result = chain.invoke({\"content\": content})\n",
    "    print(result.content)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # Save to memory\n",
    "    memory.save_context({\"content\": content}, {\"output\": result.content})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ee24d-2b51-4ace-a0c2-ee3d34b64b2d",
   "metadata": {},
   "source": [
    "## Saving Chat Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecab6934-da56-486e-8463-f67abc95a444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your prompt:  conversational buffer memory is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'conversational buffer memory is', 'chat_history': [], 'text': '\"Conversational buffer memory\" in the context of chatbots and AI refers to the ability of an AI system to retain information from earlier in the conversation and use that information to make responses more relevant and coherent. This memory helps the AI understand the context of the conversation and maintain a flow, making interactions seem more natural to human users.\\n\\nDifferent implementations of AI and chatbots may utilize various approaches to manage conversational buffer memory. For instance, some might keep track of the conversation as long as the session is active (short-term memory), while others might have capabilities to remember past interactions over longer periods (long-term memory), though they typically do this using anonymized or generalized data to preserve user privacy.\\n\\nIn practice, this memory allows the chatbot to recall what has been discussed earlier in a conversation, preventing the need to repeat information and enabling the bot to answer follow-up questions more effectively.'}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your prompt:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=False) \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Import FileChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo', temperature=1)\n",
    "\n",
    "# 2. Add an additional keyword argument to the ConversationBufferMemory() constructor\n",
    "history = FileChatMessageHistory('chat_history.json')\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    chat_memory=history,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\", \"chat_history\"],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    response = chain.invoke({'content': content})\n",
    "    print(response)\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f803b5-ab58-4f42-aa60-98c016a5c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='conversational buffer memory is', additional_kwargs={}, response_metadata={}), AIMessage(content='\"Conversational buffer memory\" in the context of chatbots and AI refers to the ability of an AI system to retain information from earlier in the conversation and use that information to make responses more relevant and coherent. This memory helps the AI understand the context of the conversation and maintain a flow, making interactions seem more natural to human users.\\n\\nDifferent implementations of AI and chatbots may utilize various approaches to manage conversational buffer memory. For instance, some might keep track of the conversation as long as the session is active (short-term memory), while others might have capabilities to remember past interactions over longer periods (long-term memory), though they typically do this using anonymized or generalized data to preserve user privacy.\\n\\nIn practice, this memory allows the chatbot to recall what has been discussed earlier in a conversation, preventing the need to repeat information and enabling the bot to answer follow-up questions more effectively.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# The messages property contains the list of messages in order.\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddcaff-88af-4e8c-9d03-5f3c78466806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
